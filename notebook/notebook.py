# -*- coding: utf-8 -*-
"""Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1djIdZ4NAAt4_-KRr1dsVQQ2cxC2j9a58

# Importing Libraries
"""

import warnings
warnings.filterwarnings('ignore')

from sklearn.feature_extraction.text import TfidfVectorizer
import re
import base64
import numpy as np
import os
import pandas as pd
from io import BytesIO
import matplotlib.pyplot as plt
import string
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import  classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import VotingClassifier
import seaborn as sns
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from string import punctuation
import nltk
from nltk.stem import WordNetLemmatizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import  MaxPooling2D
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Convolution2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import model_from_json
import pickle
from sklearn.svm import SVC
from wordcloud import WordCloud, STOPWORDS
from sklearn.ensemble import RandomForestClassifier
import tensorflow as tf
from sklearn.naive_bayes import GaussianNB

from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import  MaxPooling2D
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Convolution2D
from tensorflow.keras.models import Sequential

"""# Importing Dataset"""

df = pd.read_excel('Dataset.xlsx')

"""# EDA"""

df.head()

# df = pd.DataFrame(data)

# Get the column names
columns = df.columns

# Look for the column containing 'Target'
target_column = [col for col in columns if 'Target_Processed' in col]

if target_column:
    # Remove brackets, commas, and quotation marks
    df[target_column[0]] = df[target_column[0]].str.replace("[", "").str.replace("]", "").str.replace("'", "").str.replace(",", "")


# df['Target_Processed'] = df['Target_Processed'].str.replace("[", "").str.replace("]", "").str.replace("'", "").str.replace(",", "")
df.head()

dataset= df

dataset.info()

dataset.isnull().sum()

dataset.describe()

"""# Data Visualization"""

symptom_counts = dataset['Target_Processed'].str.split().explode().value_counts()


top_n = 10
plt.figure(figsize=(10, 6))
sns.barplot(x=symptom_counts.head(top_n).index, y=symptom_counts.head(top_n).values, palette="viridis")
plt.title('Top {} Symptoms for Fungal Infections'.format(top_n))
plt.xlabel('Symptoms')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.show()

"""# Text Cleaning"""

import nltk
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def cleanData(doc):
    tokens = doc.split()
    table = str.maketrans('', '', punctuation)
    tokens = [w.translate(table) for w in tokens]
    tokens = [word for word in tokens if word.isalpha()]
    tokens = [w for w in tokens if not w in stop_words]
    tokens = [word for word in tokens if len(word) > 1]
    tokens = [lemmatizer.lemmatize(token) for token in tokens]
    tokens = ' '.join(tokens)
    return tokens

labels = dataset['Source'].unique().tolist()
symptoms = dataset.Target_Processed
diseases = dataset.Source
Y = []
for i in range(len(diseases)):
    index = labels.index(diseases[i])
    Y.append(index)

import nltk
nltk.download('wordnet')
X = []
for i in range(len(symptoms)):
    arr = symptoms[i]
    arr = arr.strip().lower()
    arr = arr.replace("_", " ")
    X.append(cleanData(arr))

"""# Tokenization"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS as stopwords

# Assuming you have defined your X and Y variables already

# Convert the frozenset of stopwords to a list
stopwords_list = list(stopwords)

# Create a TfidfVectorizer instance
tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords_list, use_idf=True, smooth_idf=False, norm=None, decode_error='replace')

# Fit the vectorizer and transform the text data
tfidf = tfidf_vectorizer.fit_transform(X).toarray()

# Get the feature names
feature_names = tfidf_vectorizer.get_feature_names_out()

# Create a DataFrame with the tfidf values and feature names
df = pd.DataFrame(tfidf, columns=feature_names)

# Assuming you have defined your X and Y variables already
X = df
y = Y

y = Y
Y = np.asarray(Y)

X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.20, random_state = 42)

"""# Random Forest"""

RF = RandomForestClassifier(max_depth=2, random_state=0)
RF.fit(X_train, Y_train)
predictions = RF.predict(X_test)
val = (accuracy_score(Y_test, predictions)*100)
print("*Accuracy score for Random Forest: ", val, "\n")
print("*Confusion Matrix for Random Forest: ")
print(confusion_matrix(Y_test, predictions))
print("*Classification Report for Random Forest: ")
print(classification_report(Y_test, predictions))

acc = accuracy_score(Y_test, predictions) * 100
p = precision_score(Y_test, predictions,average='macro') * 100
r = recall_score(Y_test, predictions,average='macro') * 100
f = f1_score(Y_test, predictions,average='macro') * 100



print("Accuracy: {:.2f}%".format(acc))
print("Precision: {:.2f}%".format(p))
print("Recall: {:.2f}%".format(r))
print("F1-score: {:.2f}%".format(f))

"""# KNN"""

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, Y_train)
predictions = knn.predict(X_test)
val = (accuracy_score(Y_test, predictions)*100)
print("*Accuracy score for  KNN: ", val, "\n")
print("*Confusion Matrix for  KNN: ")
print(confusion_matrix(Y_test, predictions))
print("*Classification Report for KNN: ")
print(classification_report(Y_test, predictions))

acc1 = accuracy_score(Y_test, predictions) * 100
p1 = precision_score(Y_test, predictions,average='macro') * 100
r1 = recall_score(Y_test, predictions,average='macro') * 100
f1 = f1_score(Y_test, predictions,average='macro') * 100



print("Accuracy: {:.2f}%".format(acc1))
print("Precision: {:.2f}%".format(p1))
print("Recall: {:.2f}%".format(r1))
print("F1-score: {:.2f}%".format(f1))

"""# SVM"""

svm = SVC(gamma='auto')
svm.fit(X_train, Y_train)
predictions = svm.predict(X_test)
val = (accuracy_score(Y_test, predictions)*100)
print("*Accuracy score for  svm: ", val, "\n")
print("*Confusion Matrix for  svm: ")
print(confusion_matrix(Y_test, predictions))
print("*Classification Report for svm: ")
print(classification_report(Y_test, predictions))

acc2 = accuracy_score(Y_test, predictions) * 100
p2 = precision_score(Y_test, predictions,average='macro') * 100
r2 = recall_score(Y_test, predictions,average='macro') * 100
f2 = f1_score(Y_test, predictions,average='macro') * 100



print("Accuracy: {:.2f}%".format(acc2))
print("Precision: {:.2f}%".format(p2))
print("Recall: {:.2f}%".format(r2))
print("F1-score: {:.2f}%".format(f2))

"""# Gradient Boosting"""

GB = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)
GB.fit(X_train, Y_train)
predictions = GB.predict(X_test)
val = (accuracy_score(Y_test, predictions)*100)
print("*Accuracy score for  GB: ", val, "\n")
print("*Confusion Matrix for  GB: ")
print(confusion_matrix(Y_test, predictions))
print("*Classification Report for GB: ")
print(classification_report(Y_test, predictions))

acc3 = accuracy_score(Y_test, predictions) * 100
p3 = precision_score(Y_test, predictions,average='macro') * 100
r3 = recall_score(Y_test, predictions,average='macro') * 100
f3 = f1_score(Y_test, predictions,average='macro') * 100



print("Accuracy: {:.2f}%".format(acc3))
print("Precision: {:.2f}%".format(p3))
print("Recall: {:.2f}%".format(r3))
print("F1-score: {:.2f}%".format(f3))

"""# Voting Classifier"""

clf1 = KNeighborsClassifier(n_neighbors=3)
clf2 = SVC()

eclf1 = VotingClassifier(estimators=[('knn', clf1), ('svc', clf2)], voting='hard')

eclf1.fit(X_train, Y_train)
predictions = eclf1.predict(X_test)
val = (accuracy_score(Y_test, predictions)*100)
print("*Accuracy score for  Voting Classifier: ", val, "\n")
print("*Confusion Matrix for  Voting Classifier: ")
print(confusion_matrix(Y_test, predictions))
print("*Classification Report for Voting Classifier: ")
print(classification_report(Y_test, predictions))

acc4 = accuracy_score(Y_test, predictions) * 100
p4 = precision_score(Y_test, predictions,average='macro') * 100
r4 = recall_score(Y_test, predictions,average='macro') * 100
f4 = f1_score(Y_test, predictions,average='macro') * 100



print("Accuracy: {:.2f}%".format(acc4))
print("Precision: {:.2f}%".format(p4))
print("Recall: {:.2f}%".format(r4))
print("F1-score: {:.2f}%".format(f4))

"""# CNN"""

Y = np.asarray(Y)
print(Y)
X = X.values
indices = np.arange(X.shape[0])
np.random.shuffle(indices)
a = X
b = Y
X1 = a[indices]
Y1 = b[indices]
X = X[indices]
Y = Y[indices]
Y = to_categorical(Y)
X = X.reshape(X.shape[0],X.shape[1],1,1)

classifier = Sequential()
classifier.add(Convolution2D(32, 1, 1, input_shape = (X.shape[1], X.shape[2], X.shape[3]), activation = 'relu'))
classifier.add(MaxPooling2D(pool_size = (1, 1)))
classifier.add(Convolution2D(32, 1, 1, activation = 'relu'))
classifier.add(MaxPooling2D(pool_size = (1, 1)))
classifier.add(Flatten())
classifier.add(Dense(256, activation = 'relu'))
classifier.add(Dense(Y.shape[1], activation = 'softmax'))

classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
hist = classifier.fit(X, Y, batch_size=8, epochs=10, shuffle=True, verbose=2)

classifier.save('model.h5')

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)

predict = classifier.predict(X_test)
predict = np.argmax(predict, axis=1)
testY = np.argmax(y_test, axis=1)

p5 = precision_score(testY, predict,average='macro') * 100
r5 = recall_score(testY, predict,average='macro') * 100
f5 = f1_score(testY, predict,average='macro') * 100
acc5 = accuracy_score(testY,predict)*100

print("Accuracy: {:.2f}%".format(acc5))
print("Precision: {:.2f}%".format(p5))
print("Recall: {:.2f}%".format(r5))
print("F1-score: {:.2f}%".format(f5))

"""# Comparison"""

results = {
    'Accuracy': [acc, acc1, acc2, acc3, acc4, acc5],
    'Recall': [r, r1, r2, r3, r4, r5],
    'Precision': [p, p1, p2, p3, p4, p5],
    'F1': [f, f1, f2, f3, f4, f5]
}

index = ['Random Forest','KNN','SVM','Gradient Boosting','Voting Classifier',' CNN']

results =pd.DataFrame(results,index=index)
print(results)

fig =results.plot(kind='bar',title='Comaprison of models',figsize =(19,19)).get_figure()
fig.savefig('Final Result.png')

accuracy_values = results['Accuracy']
accuracy_df = pd.DataFrame({'Algorithms': index, 'Accuracy': accuracy_values})

plt.figure(figsize=(8, 6))
plt.bar(accuracy_df['Algorithms'], accuracy_df['Accuracy'], color='blue')
plt.title('Accuracy of Different Algorithms')
plt.xlabel('Algorithms')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

recall_values = results['Recall']
recall_df = pd.DataFrame({'Algorithms': index, 'Recall': recall_values})

plt.figure(figsize=(8, 6))
plt.bar(recall_df['Algorithms'], recall_df['Recall'], color='orange')
plt.title('Recall of Different Algorithms')
plt.xlabel('Algorithms')
plt.ylabel('Recall')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

precision_values = results['Precision']
precision_df = pd.DataFrame({'Algorithms': index, 'Precision': precision_values})

plt.figure(figsize=(8, 6))
plt.bar(precision_df['Algorithms'], precision_df['Precision'], color='green')
plt.title('Precision of Different Algorithms')
plt.xlabel('Algorithms')
plt.ylabel('Precision')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

f1_values = results['F1']
f1_df = pd.DataFrame({'Algorithms': index, 'F1 Score': f1_values})

plt.figure(figsize=(8, 6))
plt.bar(f1_df['Algorithms'], f1_df['F1 Score'], color='red')
plt.title('F1 Score of Different Algorithms')
plt.xlabel('Algorithms')
plt.ylabel('F1 Score')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Get the size of the dataset
dataset_size = dataset.shape[0]
print("Size of the dataset:", dataset_size)

# Get the composition of the dataset by target class
composition = dataset['Source'].value_counts()
print("\nComposition of the dataset by target class:")
print(composition)

# Define the data for the table
data = {
    'Model': ['Random Forest', 'KNN', 'SVM', 'Gradient Boosting', 'Voting Classifier', 'CNN'],
    'Training dataset accuracy (%)': [acc, acc1, acc2, acc3, acc4, acc5],  # These variables should hold the accuracy values
    'Validation accuracy (%)': ['Fill your validation accuracy', 'Fill your validation accuracy', 'Fill your validation accuracy', 'Fill your validation accuracy', 'Fill your validation accuracy', 'Fill your validation accuracy'],  # You need to replace these with actual validation accuracy values
    'Computation time': ['Fill your computation time', 'Fill your computation time', 'Fill your computation time', 'Fill your computation time', 'Fill your computation time', 'Fill your computation time']  # You need to replace these with actual computation time values
}

# Create DataFrame
result_df = pd.DataFrame(data)

# Display the DataFrame
print("Table 4: Result Analysis")
print(result_df)

import seaborn as sns
import matplotlib.pyplot as plt

# Plotting the feature map
plt.figure(figsize=(12, 8))
sns.heatmap(df.iloc[:50, :50], annot=True, cmap='viridis')  # Adjust the range [:10, :10] according to your preference
plt.title('Feature Map (TF-IDF Vectorized Features)')
plt.xlabel('Features')
plt.ylabel('Samples')
plt.show()

import matplotlib.pyplot as plt

# Model names
models = ['Random Forest', 'KNN', 'SVM', 'Gradient Boosting', 'Voting Classifier', 'CNN']

# Accuracy scores for each model
accuracy_scores = [acc, acc1, acc2, acc3, acc4, acc5]

# Precision scores for each model
precision_scores = [p, p1, p2, p3, p4, p5]

# Recall scores for each model
recall_scores = [r, r1, r2, r3, r4, r5]

# F1 scores for each model
f1_scores = [f, f1, f2, f3, f4, f5]

# Plotting
plt.figure(figsize=(12, 8))

plt.plot(models, accuracy_scores, label='Accuracy', marker='o')
plt.plot(models, precision_scores, label='Precision', marker='s')
plt.plot(models, recall_scores, label='Recall', marker='^')
plt.plot(models, f1_scores, label='F1 Score', marker='x')

plt.title('Model Performance Comparison')
plt.xlabel('Models')
plt.ylabel('Score')
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)
plt.tight_layout()

plt.show()